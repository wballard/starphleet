#!/usr/bin/env starphleet-launcher
### Usage:
###    starphleet-reaper <current_service_name> <order> [--force]
### --help
###
### Kill off every running service for an order except the current service

# Reap any containers
for name in $(lxc-ls | grep --extended-regexp -e "^${order}-([a-f0-9]){7}-([a-f0-9]){7}" | grep --invert-match "${current_service_name}")
do

  STATUS_FILE="${CURRENT_ORDERS}/${order}/.starphleetstatus.${name}"

  # If the status file exists get the status
  [ -f "${STATUS_FILE}" ] && CURRENT_STATUS=$(cat "${STATUS_FILE}")
  # Get the latest successful container deployment
  CURRENT_CONTAINER=$(cat ${CURRENT_ORDERS}/${order}/.container)
  # Determine where nginx is pointing
  NGINX_CONTAINER=$(curl --connect-timeout 1 -s -XHEAD -i "http://localhost/${order}/" | grep "X-Starphleet-Container" | cut -f 2 -d " " | tr -dc '[[:print:]]')

  # XXX: After this feature has been live a bit we can purge this...
  #      For now, I'd like to quickly be able to resort to this Even
  #      in production for quick debugging
  if dev_mode ; then
    log "NM: |${name}|"
    log "OR: |${order}|"
    log "SN: |${current_service_name}|"
    log "CS: |${CURRENT_STATUS}|"
    log "SF: |${STATUS_FILE}|"
    log "CC: |${CURRENT_CONTAINER}|"
    log "NC: |${NGINX_CONTAINER}|"
  fi

  # *****************************
  # Guards
  # *****************************
  # The following are a set of guards to prevent reaps from happening when we
  # don't want them to happen

  # Starphleet intentionally won't reap containers that fail to build
  # until a container for the same service successfully builds.  This mechanism
  # intentionally leaves around failed containers for debug purposes.  To facilitate
  # this we check if the .container file (updates on success) is 'newer' than
  # the status file of the container we are attempting to reap
  if [ "${CURRENT_ORDERS}/${order}/.container" -ot "${STATUS_FILE}" ] &&
     [ "${force}" != "true" ]; then
     info "Unable to reap ${name} - Latest container ${CURRENT_CONTAINER} is not newer"
    continue;
  fi

  # Only reap non-building containers.  Being explicit about reap conditions
  # so future statuses don't trigger accidental reapz
  # We won't reap if all are true:
  #   - The container is not online
  #   - The container is not failed
  #   - The container is not stopped
  #   - The container is not failed to publish
  if [ "${CURRENT_STATUS}" != "" ] &&
     [ "${CURRENT_STATUS}" != "online" ] &&
     [ "${CURRENT_STATUS}" != "stopped" ] &&
     [ "${CURRENT_STATUS}" != "failed" ] &&
     [ "${CURRENT_STATUS}" != "building failed" ] &&
     [ "${CURRENT_STATUS}" != "publish failed" ] &&
     [ "${force}" != "true" ]; then
     info "Unable to reap ${name} due to status: ${CURRENT_STATUS}"
    continue;
  fi

  # We won't reap if any are true:
  #   - The current active container is being attempted
  #   - NGINX is pointing at this container
  #   - Force is not enabled
  if ([ "${CURRENT_CONTAINER}" == "${name}" ] ||
     [ "${NGINX_CONTAINER}" == "${name}" ]) &&
     [ "${force}" != "true" ]; then
     info "Unable to reap ${name} - Active Container ${CURRENT_CONTAINER} - NGINX Pointing at ${NGINX_CONTAINER}"
    continue;
  fi

  info "Reaping ${name}"
  info "Stopping Upstart job ${name}"
  stop starphleet_serve_order name="${name}" || true
  info "Destroying container ${name}"
  starphleet-lxc-destroy "${name}" || true
  info "Removing status files"
  rm ${CURRENT_ORDERS}/${order}/.starphleetstatus.${name}* || true
  info "Removing Logs for ${name}"
  rm /var/log/upstart/starphleet_serve_order-${name}* || true
  rm /var/log/upstart/starphleet_orders_healthcheck-${name}* || true
  info "reaper has reaped ${name}"
done

# After we destroy all containers above, technically, all status files would
# be purged.  However, if a user manually destroys a container and it doesn't
# run through the proper reaper process it would leave around stale status
# files which would trigger the reaper over-and-over.
#
# So, check if any status files are left over and if there isn't a container
# running for them we clean them up
# Clean up any stale status files
for CONTAINER_STATUS_FILE in $(find "${CURRENT_ORDERS}/${order}/" -type f -name ".starphleetstatus*" | grep --extended-regexp -e ".*-([a-f0-9]){7}$" | grep -v "${current_service_name}")
do
    # Yoink just the container name off the status file
    container_name=$(basename "${CONTAINER_STATUS_FILE}" | sed -e 's/.starphleetstatus.//')

    # Check if there's a container associated with this status file
    check=$(lxc-ls | grep "${container_name}")

    info "Checking if status file stale: ${CONTAINER_STATUS_FILE}"

    if [ -z "${check}" ]; then
      warn "Stale Status File Detected: ${CONTAINER_STATUS_FILE}"
      rm ${CONTAINER_STATUS_FILE}*
    fi
done
