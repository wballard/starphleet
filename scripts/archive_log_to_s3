#!/usr/bin/env starphleet-launcher
### Script to take log files and push them up to S3 for later reporting
### Usage:
###   archive_log_to_s3 <path_to_logfile> <category>
# Setup from logrotate config 'lastaction' block. 
## lastaction
##  archive_log_to_s3 $1 nginx
## endscript
#
# Requires configuration of AWS creds which needs to include:
export AWS_ACCESS_KEY_ID=${PHLEET_LOGS_ARCHIVE_AWS_ACCESS_KEY_ID}
export AWS_SECRET_ACCESS_KEY=${PHLEET_LOGS_ARCHIVE_AWS_SECRET_ACCESS_KEY}
export AWS_DEFAULT_REGION=${PHLEET_LOGS_ARCHIVE_AWS_DEFAULT_REGION:-us-west-2}
# this serves as an optional server (ship) level setting that can be used to further split the
# logs up into groups like, staging, production, prodiction-ship1, etc.
export LOG_ARCHIVE_PREFIX=${PHLEET_LOGS_ARCHIVE_PREFIX}
export LOG_ARCHIVE_BUCKET=${PHLEET_LOGS_ARCHIVE_BUCKET?This really doesnt work without a bucket name}

# If aws credential are not setup, or there is an AWS failure, the action fails but does not affect the log rotate action.

# Spaces are not allowed in category 
category=$(echo "${category}" | sed 's/[[:space:]]//g')

# Create a unique file name to store up to s3 to include category+hostname+timestamp
extension="$(hostname)_$(date -u "+%m-%d-%Y_%H:%M:%S")_$AWS_REGION.log.gz"
cp_to_file="$(basename -s '.log.1.gz' $path_to_logfile).$extension"

if [ -n "${LOG_ARCHIVE_PREFIX}" ]; then
  aws s3 cp "$path_to_logfile" "s3://${LOG_ARCHIVE_BUCKET}/${LOG_ARCHIVE_PREFIX}/${category}/$cp_to_file" &>> /var/log/archive_to_s3_for_$category.log
else
  aws s3 cp "$path_to_logfile" "s3://${LOG_ARCHIVE_BUCKET}/${category}/$cp_to_file" &>> /var/log/archive_to_s3_for_$category.log
fi
